{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acef4c1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ee24e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datetime.today() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f6e7e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2022.05.17', '2021.05.17')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = datetime.today()\n",
    "last = now - relativedelta(years=1)\n",
    "\n",
    "now = now.strftime('%Y.%m.%d')\n",
    "last = last.strftime('%Y.%m.%d')\n",
    "\n",
    "now, last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c42955a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14, 14, 14)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample\n",
    "\n",
    "title_list = []\n",
    "date_list = []\n",
    "article_list = []\n",
    "link_list = []\n",
    "sub_list = []\n",
    "\n",
    "subject = '개인연금'\n",
    "start = 1\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\"}\n",
    "url = f'https://search.naver.com/search.naver?where=news&sm=tab_pge&query={subject}&sort=0&photo=0&field=0&pd=5&ds={last}&de={now}&cluster_rank=51&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:1y,a:all&start={start}'\n",
    "\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "ul = soup.find('ul', {'class': 'list_news'})\n",
    "links = ul.find_all('a', {'class': 'info'})\n",
    "\n",
    "for link in links:    \n",
    "    if '네이버' in link.text:\n",
    "        url_a = link['href']\n",
    "        res_a = requests.get(url_a, headers=headers)\n",
    "        soup_a = BeautifulSoup(res_a.text, 'lxml')\n",
    "        title = soup_a.find('h2', {'class': 'media_end_head_headline'})\n",
    "        article = soup_a.find('div', {'class': 'newsct_article'})\n",
    "        date = soup_a.find('span', {'class': 'media_end_head_info_datestamp_time _ARTICLE_DATE_TIME'})\n",
    "        article = article.text.replace('\\t\\t', '\\n\\n').split('\\n\\n')\n",
    "        for p in article:\n",
    "            p = p.replace('\\n', ' ').replace('\\t', '').replace('\\xa0', ' ').strip()\n",
    "            if len(p) < 5:\n",
    "                continue\n",
    "            else:\n",
    "                title_list.append(title.text)\n",
    "                date_list.append(date.text)\n",
    "                article_list.append(p)\n",
    "                sub_list.append(subject)\n",
    "\n",
    "len(title_list), len(date_list), len(article_list), len(sub_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0966e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = ['ETF', 'IRP', '연금저축', '연금상품',\n",
    "          '증권', '수익률', '수령', '납입', '한도',\n",
    "          '이전', '사망', '노후', '출금', '세제',\n",
    "          '연령', '세대', '2030', '퇴직',\n",
    "          '국민연금', '연금개혁', '운용', '펀드',\n",
    "          '종목', '가입', '수수료', '가입서류', '연금계좌',\n",
    "          '원금보장', '비교', '해지']\n",
    "\n",
    "base = ['\"개인연금\"', '\"퇴직연금\"']\n",
    "subjects = base + [f'{b} +'+s for b in base for s in search]\n",
    "subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ef8c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "''.join(subjects[-1].replace('\"', '').split('+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c3b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['title', 'article', 'link', 'sub'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bae2d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "link_list = []\n",
    "\n",
    "for subject in subjects:\n",
    "    for i in range(0, 10):\n",
    "        start = 1 + (i*10)\n",
    "    \n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\"}\n",
    "        url = f'https://search.naver.com/search.naver?where=news&sm=tab_pge&query={subject}&sort=0&photo=0&field=0&pd=5&ds={last}&de={now}&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:1y,a:all&start={start}'\n",
    "\n",
    "        res = requests.get(url)\n",
    "        res.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "        ul = soup.find('ul', {'class': 'list_news'})\n",
    "        links = ul.find_all('a', {'class': 'info'})\n",
    "\n",
    "        for link in links:\n",
    "            if link not in link_list and '네이버' in link.text:\n",
    "                url_a = link['href']\n",
    "                try:\n",
    "                    res_a = requests.get(url_a, headers=headers)\n",
    "                    soup_a = BeautifulSoup(res_a.text, 'lxml')\n",
    "                    title = soup_a.find('h2', {'class': 'media_end_head_headline'})\n",
    "                    article = soup_a.find('div', {'class': 'newsct_article'})\n",
    "                    article = article.text.replace('\\t\\t', '\\n\\n').split('\\n\\n')\n",
    "                    for p in article:\n",
    "                        p = p.replace('\\n', ' ').replace('\\t', '').replace('\\xa0', ' ').strip()\n",
    "                        if len(p) < 30:\n",
    "                            continue\n",
    "                        else:\n",
    "                            temp = dict()\n",
    "                            temp['title'] = title.text\n",
    "                            temp['article'] = p\n",
    "                            temp['link'] = url_a\n",
    "                            temp['sub'] = ''.join(subject.replace('\"', '').split('+'))\n",
    "                            df.loc[len(df)] = temp\n",
    "                except:\n",
    "                    pass\n",
    "                link_list.append(url_a)\n",
    "                \n",
    "    print(f'{subject} is done!')\n",
    "print('END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadece39",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(title_list), len(link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9c90b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('../data/news_double.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbe187c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crypto",
   "language": "python",
   "name": "crypto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
